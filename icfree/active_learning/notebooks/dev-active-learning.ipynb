{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ahead of using this notebook\n",
    "---\n",
    "### Pre-Processing raw plate reader outputs\n",
    "---\n",
    "The user provides his plate reader fluorescence files. These files go through a 2-steps process to be \"active-learning\"-ready:\n",
    "\n",
    "* **Step 1:** generate a first dataframe with these columns: components concentrations, replicas well names, replicas measured fluorescence. [Example File](https://github.com/brsynth/icfree-ml/blob/yorgo/tests/data/active_learning/input/experimental_data/raw_all_fluorescence.csv)\n",
    "\n",
    "* **Step 2:** generate a second dataframe with these columns: components concentrations (X_data), mean of replicas measured fluorescence (y_data), std of replicas measured fluorescence data (y_std_data). [Example File](https://github.com/brsynth/icfree-ml/blob/yorgo/tests/data/active_learning/input/experimental_data/plate_AL_1_fluorescence_and_std.csv)\n",
    "\n",
    "**Notice**\n",
    "- Currently, the formatting process is conducted manually, the plan is to implement a \"pre-processing\" function that automates the above-mentioned steps.\n",
    "\n",
    "- **Bottelnecks:** plate reader outputs are not universal and mutliple fluorescence gains could be reported in a single file which makes parsing case-specific. The user will still need to manually extract and store in a file the measured fluorescence he wishes to study. That file would go through the above-mentioned steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "from csv import (\n",
    "    DictWriter as csv_DictWriter\n",
    ")\n",
    "\n",
    "from typing import (\n",
    "    List\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "from pandas import (\n",
    "    read_csv as pd_read_csv\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    r2_score\n",
    ")\n",
    "\n",
    "from sklearn.neural_network import (\n",
    "    MLPRegressor\n",
    ")\n",
    "\n",
    "# from icfree.plates_generator.plates_generator import (\n",
    "#     input_importer,\n",
    "#     input_processor,\n",
    "#     doe_levels_generator,\n",
    "#     levels_to_concentrations\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active Learning Workflow\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 1:** provide a data folder and the number of files to merge together to generate a training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_generator(\n",
    "        data_folder: str,\n",
    "        files_number: int\n",
    "):\n",
    "    \"\"\"\n",
    "    Merge experimental data into a single dataset\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        data_folder: str\n",
    "            Path to folder with experimental data\n",
    "        files_number: int\n",
    "            Number of files to be merged into a single dataset\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        dataset: ndarray\n",
    "            Array with all experimental data\n",
    "    \"\"\"\n",
    "    # Read file(s) and store in dataframe(s)\n",
    "    experimental_data = [pd_read_csv(\n",
    "            data_folder.format(i))\n",
    "            for i in range(1, files_number + 1)]\n",
    "\n",
    "    # Concatenate all dataframes\n",
    "    dataset = np.concatenate(\n",
    "        experimental_data,\n",
    "        axis=0)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# Call function\n",
    "data_folder = '/Users/yorgo/Documents/GitHub/icfree-ml/tests/data/active_learning/input/experimental_data_borkowski_et_al/plate_AL_{}_raw_yield_and_std.csv'\n",
    "# data_folder = input(\"Enter the file path: \")\n",
    "files_numbers = 3\n",
    "dataset = dataset_generator(data_folder, files_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporary chunk of code\n",
    "For Paul's project we currently have one dataset only (imported below), once more are generated, the above function can be executed and the current section can be discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_experimental_data = '/Users/yorgo/Documents/GitHub/icfree-ml/tests/data/active_learning/input/experimental_data/plate_AL_1_fluorescence_and_std_TEST.csv'\n",
    "# tmp_experimental_data = input(\"Enter the file path: \")\n",
    "dataset = np.genfromtxt(\n",
    "    tmp_experimental_data,\n",
    "    delimiter=',',\n",
    "    skip_header=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 2:** the training dataset is processed to extract elements: y_data, y_std_data, X_data and max_X_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_processor(dataset):\n",
    "    \"\"\"\n",
    "    Extract X and Y data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        dataset: ndarray\n",
    "            Array with all experimental data\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        X_data: ndarray\n",
    "            Array of X_data\n",
    "        y_data: ndarray\n",
    "            Array of mean y_data\n",
    "        y_std_data: ndarray\n",
    "            Array of standard deviation of y_data\n",
    "        max_X_data: List\n",
    "            List of maximum X_data values\n",
    "    \"\"\"\n",
    "    # Extract X_data\n",
    "    X_data = dataset[:, 0:3]\n",
    "\n",
    "    # Extract y_data\n",
    "    y_data = dataset[:, 3]\n",
    "\n",
    "    # Extract y_std_data\n",
    "    y_std_data = dataset[:, 4]\n",
    "\n",
    "    # Extract maximimum X_data values for nomalisation\n",
    "    max_X_data = []\n",
    "    for i in range(X_data.shape[1]):\n",
    "        max_X_data.append(\n",
    "            copy.deepcopy(\n",
    "                max(X_data[:, i])))\n",
    "        X_data[:, i] = X_data[:, i]/max(X_data[:, i])\n",
    "\n",
    "    return (X_data,\n",
    "            y_data,\n",
    "            y_std_data,\n",
    "            max_X_data)\n",
    "\n",
    "# Call function\n",
    "X_data, y_data, y_std_data, max_X_data = dataset_processor(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 3:** generate an esemble of pre-trained regressor models. A selection process is applied while building the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_single_regressor(\n",
    "    X_data,\n",
    "    y_data,\n",
    "    model_name: str,\n",
    "    models_number: int\n",
    "):\n",
    "    \"\"\"\n",
    "    Train and select regressor models with different hidden layers sizes\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        X_data: ndarray\n",
    "            Array of X_data\n",
    "        y_data: ndarray\n",
    "            Array of mean y_data\n",
    "        models_number: int\n",
    "            Number of models to create\n",
    "        model_name: str\n",
    "            Name of the model\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        best_model: MLPRegressor\n",
    "            Multi-layer Perceptron regressor\n",
    "        best_score: numpy.float64\n",
    "            Coefficient of determination (r2 score)\n",
    "    \"\"\"\n",
    "    # Train the models\n",
    "    trained_models = []\n",
    "    for i in range(models_number):\n",
    "        X_train = X_data\n",
    "        y_train = y_data\n",
    "        \n",
    "        regressor1 = MLPRegressor(\n",
    "                hidden_layer_sizes=(10, 100, 100, 20),\n",
    "                activation=\"relu\",\n",
    "                solver=\"adam\",\n",
    "                max_iter=20000,\n",
    "                early_stopping=True,\n",
    "                learning_rate=\"adaptive\")\n",
    "        regressor1.fit(X_train, y_train.flatten())\n",
    "        trained_models.append(regressor1)\n",
    "\n",
    "        regressor2 = MLPRegressor(\n",
    "                hidden_layer_sizes=(20, 100, 100, 10),\n",
    "                activation=\"relu\",\n",
    "                solver=\"adam\",\n",
    "                max_iter=20000,\n",
    "                early_stopping=True,\n",
    "                learning_rate=\"adaptive\")\n",
    "        regressor2.fit(X_train, y_train.flatten())\n",
    "        trained_models.append(regressor2)\n",
    "\n",
    "        regressor3 = MLPRegressor(\n",
    "                hidden_layer_sizes=(100, 50, 50, 40),\n",
    "                activation=\"relu\",\n",
    "                solver=\"adam\",\n",
    "                max_iter=20000,\n",
    "                early_stopping=True,\n",
    "                learning_rate=\"adaptive\")\n",
    "        regressor3.fit(X_train, y_train.flatten())\n",
    "        trained_models.append(regressor3)\n",
    "\n",
    "        regressor4 = MLPRegressor(\n",
    "                hidden_layer_sizes=(40, 50, 50, 100),\n",
    "                activation=\"relu\",\n",
    "                solver=\"adam\",\n",
    "                max_iter=20000,\n",
    "                early_stopping=True,\n",
    "                learning_rate=\"adaptive\")\n",
    "        regressor4.fit(X_train, y_train.flatten())\n",
    "        trained_models.append(regressor4)\n",
    "\n",
    "    # Evaluate the output of the models\n",
    "    all_scores = []\n",
    "    for i in range(len(trained_models)):\n",
    "        y_pred = trained_models[i].predict(X_data)\n",
    "        score = r2_score(y_data, y_pred)\n",
    "        all_scores.append(score)\n",
    "    try:\n",
    "        best_index = all_scores.index(max(all_scores))\n",
    "        best_score = all_scores[best_index]\n",
    "    except ValueError:\n",
    "        best_index = 0\n",
    "    best_model = trained_models[best_index]\n",
    "\n",
    "    # Visualize the output of the model\n",
    "    model = best_model\n",
    "    y_pred = model.predict(X_data)\n",
    "    score = r2_score(y_data, y_pred)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(y_data, y_pred, edgecolors=(0, 0, 0))\n",
    "    ax.plot(\n",
    "        [y_data.min(), y_data.max()],\n",
    "        [y_data.min(), y_data.max()], 'k--', lw=4)\n",
    "    ax.set_xlabel(\"Measured\")\n",
    "    ax.set_title(\n",
    "        \"Model prediction for model {}: {}\".format(model_name, score))\n",
    "    ax.set_ylabel(\"Predicted\")\n",
    "    plt.show()\n",
    "\n",
    "    return (best_model, best_score)\n",
    "\n",
    "def create_ensemble_regressor(\n",
    "        X_data,\n",
    "        y_data,\n",
    "        y_std_data,\n",
    "        ensemble_size: int\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate a list of pre-trained regressor models\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        X_data: ndarray\n",
    "            Array of X_data\n",
    "        y_data: ndarray\n",
    "            Array of mean y_data\n",
    "        y_std_data: ndarray\n",
    "            Array of standard deviation of y_data\n",
    "        ensemble_size: int\n",
    "            Number of regressor models to add in the ensemble\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        MLP_ensemble: List\n",
    "            List of pre-trained regressor models\n",
    "    \"\"\"\n",
    "    # Train the ensemble of models\n",
    "    MLP_ensemble = []\n",
    "    for i in range(ensemble_size):\n",
    "        model, score = create_single_regressor(\n",
    "            X_data,\n",
    "            y_data,\n",
    "            models_number=10,\n",
    "            model_name=i)\n",
    "        MLP_ensemble.append(model)\n",
    "\n",
    "    # Evaluate and visualize the ouputs of models\n",
    "    for i in range(len(MLP_ensemble)):\n",
    "        model = MLP_ensemble[i]\n",
    "        y_pred = model.predict(X_data)\n",
    "        score = r2_score(y_data, y_pred)\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.scatter(y_data, y_pred, edgecolors=(0, 0, 0))\n",
    "        ax.errorbar(y_data, y_pred, xerr=y_std_data, ls=\"none\")\n",
    "        ax.plot(\n",
    "            [y_data.min(), y_data.max()],\n",
    "            [y_data.min(), y_data.max()],\n",
    "            \"k--\",\n",
    "            lw=4)\n",
    "        ax.set_xlabel(\"Measured\")\n",
    "        ax.set_title(\"Model prediction for model {}: {}\".format(i, score))\n",
    "        ax.set_ylabel(\"Predicted\")\n",
    "        plt.show()\n",
    "\n",
    "    all_predictions = None\n",
    "    for model in MLP_ensemble:\n",
    "        y_pred = model.predict(X_data)\n",
    "        # answer_array_pred = y_pred.reshape(X_data.shape[0], -1)\n",
    "        if all_predictions is None:\n",
    "            all_predictions = y_pred.reshape(X_data.shape[0], -1)\n",
    "        else:\n",
    "            all_predictions = np.concatenate((\n",
    "                all_predictions,\n",
    "                y_pred.reshape(\n",
    "                    X_data.shape[0],\n",
    "                    -1)),\n",
    "                axis=1)\n",
    "\n",
    "    y_pred = np.mean(all_predictions, axis=1)\n",
    "    y_pred_std = np.std(all_predictions, axis=1)\n",
    "    score = r2_score(y_data, y_pred)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(y_data, y_pred, edgecolors=(0, 0, 0))\n",
    "    ax.errorbar(y_data, y_pred, xerr=y_std_data, yerr=y_pred_std, ls=\"none\")\n",
    "    ax.plot(\n",
    "            [y_data.min(), y_data.max()],\n",
    "            [y_data.min(), y_data.max()],\n",
    "            'k--',\n",
    "            lw=4)\n",
    "    ax.set_xlabel(\"Measured\")\n",
    "    ax.set_title(\"Model prediction for ensemble of models: {}\".format(score))\n",
    "    ax.set_ylabel(\"Predicted\")\n",
    "    plt.show()\n",
    "\n",
    "    return MLP_ensemble\n",
    "\n",
    "# Call function\n",
    "MLP_ensemble = create_ensemble_regressor(\n",
    "        X_data,\n",
    "        y_data,\n",
    "        y_std_data,\n",
    "        ensemble_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 4:** select new samples to test from a pool of samples using a set of pre-trained models while avoiding duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_samples(\n",
    "    new_sample,\n",
    "    array_to_avoid,\n",
    "    size: int\n",
    "):\n",
    "    \"\"\"\n",
    "    Check if a sample is present in an array\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    new_sample: ndarray\n",
    "        Array of samples to generate\n",
    "    array_to_avoid: ndarray\n",
    "        Array of samples to avoid duplicating\n",
    "    size: int\n",
    "        Number of elements in each sample\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    present: bool\n",
    "        True if the sample is present in the array, False otherwise\n",
    "    index: int\n",
    "        Index of the sample in the array\n",
    "    \"\"\"\n",
    "    present = False\n",
    "    new_sample = np.reshape(np.array(new_sample), (1, size))\n",
    "    for index in range(array_to_avoid.shape[0]):\n",
    "        if np.array_equiv(\n",
    "            array_to_avoid[index, :],\n",
    "                new_sample):\n",
    "            present = True\n",
    "            break\n",
    "    return(present, index)\n",
    "\n",
    "# Maximum concentrations for variable each component\n",
    "benzamid_max = 1000000\n",
    "hippurate_max = 100000\n",
    "coce_dna_max = 10\n",
    "\n",
    "# Maximum concentrations for each constant component\n",
    "rna_max = 3000\n",
    "enzyme_bzd_max = 10\n",
    "cocaine_max = 100000\n",
    "\n",
    "# For the first dataset (the one imported from the database), these concentrations ratios are used\n",
    "benzamid_conc = [0, 0.02, 1]\n",
    "hippurate_conc = [0, 0.02, 1]\n",
    "coce_dna_conc = [0, 0.02, 1]\n",
    "\n",
    "def generate_random_grid(\n",
    "    array_to_avoid,\n",
    "    samples_number: int,\n",
    "    normalisation: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate a random array of samples\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    array_to_avoid: ndarray\n",
    "        Array of samples to avoid duplicating\n",
    "    samples_number: int\n",
    "        Number of samples to generate\n",
    "    normalisation: str\n",
    "        True if the samples are to be normalised, False otherwise\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        active_learning_array: ndarray\n",
    "            Array of samples\n",
    "    \"\"\"\n",
    "    active_learning_array = None\n",
    "    generated_samples_number = 0\n",
    "    # while generated_samples_number < samples_number:\n",
    "    #     samples_levels = np.concatenate(\n",
    "    #         (\n",
    "    #     np.random.randint(\n",
    "    #         0,\n",
    "    #         high=3,\n",
    "    #         size=3,\n",
    "    #         dtype='int'),\n",
    "            \n",
    "    #         np.random.randint(\n",
    "    #             0,\n",
    "    #             high=2,\n",
    "    #             size=3,\n",
    "    #             dtype='int')\n",
    "    #             ),\n",
    "    #             axis=0\n",
    "    #             )\n",
    "\n",
    "    while generated_samples_number < samples_number:\n",
    "        samples_levels = np.random.randint(\n",
    "            0,\n",
    "            high=3,\n",
    "            size=3,\n",
    "            dtype='int')\n",
    "\n",
    "        if normalisation is True:\n",
    "            samples_concentrations = [\n",
    "                coce_dna_conc[samples_levels[0]],\n",
    "                benzamid_conc[samples_levels[1]],\n",
    "                hippurate_conc[samples_levels[2]]\n",
    "                ]\n",
    "        else:\n",
    "            samples_concentrations = [\n",
    "                coce_dna_conc[samples_levels[0]] * coce_dna_max,\n",
    "                benzamid_conc[samples_levels[1]] * benzamid_max,\n",
    "                hippurate_conc[samples_levels[2]] * hippurate_max\n",
    "                ]\n",
    "        samples_concentrations = np.reshape(\n",
    "            samples_concentrations,\n",
    "            (1, 3))\n",
    "\n",
    "        if not check_samples(\n",
    "            samples_concentrations,\n",
    "            array_to_avoid,\n",
    "            size=3)[0]:\n",
    "            generated_samples_number = generated_samples_number + 1\n",
    "\n",
    "            if active_learning_array is None:\n",
    "                active_learning_array = samples_concentrations\n",
    "            else:\n",
    "                active_learning_array = np.concatenate(\n",
    "                    (active_learning_array,\n",
    "                        samples_concentrations),\n",
    "                    axis=0)\n",
    "            array_to_avoid = np.concatenate(\n",
    "                (array_to_avoid,\n",
    "                    samples_concentrations),\n",
    "                axis=0)\n",
    "    return active_learning_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_model_predictions_selector(\n",
    "    ensemble_of_models: List,\n",
    "    array_to_avoid,\n",
    "    pool_size: int,\n",
    "    samples_number: int,\n",
    "    exploitation: int,\n",
    "    exploration: float,\n",
    "    normalisation: str,\n",
    "    max_X_data: List[float],\n",
    "    verbose: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Select new samples to test from a pool of samples using a set of pre-trained models\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        ensemble_of_models: List\n",
    "            List of pre-trained models\n",
    "        array_to_avoid: ndarray\n",
    "            Array of samples to avoid sampling again\n",
    "        pool_size: int\n",
    "            Total number of samples in the pool\n",
    "        samples_number: int\n",
    "            Number of samples to select and export\n",
    "        exploitation: int\n",
    "            Coefficient of focus on higher y_data query\n",
    "        exploration: float\n",
    "            Coefficient of focus on a more informative query\n",
    "        normalisation: str\n",
    "            True if the samples are to be normalised, False otherwise\n",
    "        max_X_data: List\n",
    "            List of maximum X_data values\n",
    "        verbose: str\n",
    "            If True, print progress. If False, don't print progress.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        conditions_to_test: ndarray\n",
    "            Array to maximise y_data and y_std_data\n",
    "        conditions_to_test_exploration: ndarray\n",
    "            Array to maximise y_std_data\n",
    "        conditions_to_test_exploitation: ndarray\n",
    "            Array to maximise y_data\n",
    "    \"\"\"\n",
    "    # Tested X_data\n",
    "    active_learning_array = generate_random_grid(\n",
    "        array_to_avoid,\n",
    "        samples_number=pool_size,\n",
    "        normalisation=normalisation)\n",
    "\n",
    "    # Predicted X_data\n",
    "    all_predictions = None\n",
    "    for model in ensemble_of_models:\n",
    "        y_pred = model.predict(active_learning_array)\n",
    "        if all_predictions is None:\n",
    "            all_predictions = y_pred.reshape(\n",
    "                active_learning_array.shape[0],\n",
    "                -1)\n",
    "        else:\n",
    "            all_predictions = np.concatenate(\n",
    "                (all_predictions,\n",
    "                 y_pred.reshape(\n",
    "                     active_learning_array.shape[0],\n",
    "                     -1)),\n",
    "                axis=1)\n",
    "\n",
    "    # Compute mean of the predicted array\n",
    "    y_pred = np.mean(\n",
    "        all_predictions,\n",
    "        axis=1)\n",
    "\n",
    "    # Compute std of the predicted array\n",
    "    y_pred_std = np.std(\n",
    "        all_predictions,\n",
    "        axis=1)\n",
    "\n",
    "    # Array to maximise, while balancing between exploration and exploitation\n",
    "    array_to_maximise = copy.deepcopy(\n",
    "        exploitation * y_pred + exploration * y_pred_std)\n",
    "\n",
    "    # Select arrays in regards of the exploration method\n",
    "    # Select arrays in regards of y_pred only\n",
    "    conditions_list_exploitation = []\n",
    "    for count in range(samples_number):\n",
    "        i = np.argmax(y_pred)\n",
    "        conditions_list_exploitation.append(int(i))\n",
    "        if verbose:\n",
    "            print(\n",
    "                \"Maximising sample {} is yield: {}, std = {}\".format(\n",
    "                    i,\n",
    "                    y_pred[i],\n",
    "                    y_pred_std[i])\n",
    "            )\n",
    "        y_pred[i] = -1\n",
    "\n",
    "    # Select arrays in regards of y_pred_std only\n",
    "    conditions_list_exploration = []\n",
    "    for count in range(samples_number):\n",
    "        i = np.argmax(y_pred_std)\n",
    "        conditions_list_exploration.append(int(i))\n",
    "        if verbose:\n",
    "            print(\n",
    "                \"Maximising sample {} is yield: {}, std = {}\".format(\n",
    "                    i,\n",
    "                    y_pred[i],\n",
    "                    y_pred_std[i])\n",
    "            )\n",
    "        y_pred_std[i] = -1\n",
    "\n",
    "    # Select arrays in regards of both y_pred and y_pred_std\n",
    "    conditions_list = []\n",
    "    for count in range(samples_number):\n",
    "        i = np.argmax(array_to_maximise)\n",
    "        conditions_list.append(int(i))\n",
    "        if verbose:\n",
    "            print(\n",
    "                \"Maximising sample {} is yield: {}, std = {}\".format(\n",
    "                    i,\n",
    "                    y_pred[i],\n",
    "                    y_pred_std[i])\n",
    "            )\n",
    "        array_to_maximise[i] = -1\n",
    "\n",
    "    # Normalise X_data values\n",
    "    if normalisation is True:\n",
    "        for i in range(active_learning_array.shape[1]):\n",
    "            active_learning_array[:, i] = \\\n",
    "                active_learning_array[:, i] * max_X_data[i]\n",
    "    # Don't normalise X_data values\n",
    "    else:\n",
    "        active_learning_array = active_learning_array\n",
    "\n",
    "    # Generate conditions to test exploration and exploitation\n",
    "    conditions_to_test = active_learning_array[conditions_list, :]\n",
    "\n",
    "    # Generate conditions to test exploration\n",
    "    conditions_to_test_exploration = \\\n",
    "        active_learning_array[conditions_list_exploration, :]\n",
    "\n",
    "    # Generate conditions to test exploitation\n",
    "    conditions_to_test_exploitation = \\\n",
    "        active_learning_array[conditions_list_exploitation, :]\n",
    "\n",
    "    return(\n",
    "        conditions_to_test,\n",
    "        conditions_to_test_exploration,\n",
    "        conditions_to_test_exploitation\n",
    "        )\n",
    "\n",
    "# Call function\n",
    "conditions_to_test, \\\n",
    "conditions_to_test_exploration, \\\n",
    "conditions_to_test_exploitation = \\\n",
    "ensemble_model_predictions_selector(\n",
    "    ensemble_of_models=MLP_ensemble,\n",
    "    array_to_avoid=X_data,\n",
    "    pool_size=21,\n",
    "    samples_number=10,\n",
    "    exploitation=1,\n",
    "    exploration=1.41,\n",
    "    normalisation=True,\n",
    "    max_X_data=max_X_data,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 5:** post-process and save generated dataframes samples to fit as echo instructor input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_conditions_to_test(\n",
    "    array,\n",
    "    output_file,\n",
    "    normalisation\n",
    "):\n",
    "    \"\"\"\n",
    "    Save arrays in CSV files\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    array: ndarray\n",
    "        Array to save\n",
    "    output_file: str\n",
    "        Path to output file\n",
    "    normalisation: str\n",
    "        True if the samples are to be normalised, False otherwise\n",
    "    \"\"\"\n",
    "    fieldnames = [\n",
    "        \"CocE DNA\",\n",
    "        \"Benzamid\",\n",
    "        \"Hippurate\",\n",
    "        \"RNA\",\n",
    "        \"Enzyme BZD\",\n",
    "        \"Cocaine\",\n",
    "        \"Extract/Buffer\",\n",
    "        \"DNA-Mix\"\n",
    "        ]\n",
    "\n",
    "    conditions = []\n",
    "    for row in array:\n",
    "        conditions_1_rows = {}\n",
    "        conditions_1_rows[\"CocE DNA\"] = round(float(row[0]), 5)\n",
    "        conditions_1_rows[\"Benzamid\"] = round(float(row[1]), 5)\n",
    "        conditions_1_rows[\"Hippurate\"] = round(float(row[2]), 5)\n",
    "\n",
    "        if normalisation is True:\n",
    "            conditions_1_rows[\"RNA\"] = 1\n",
    "            conditions_1_rows[\"Enzyme BZD\"] = 1\n",
    "            conditions_1_rows[\"Cocaine\"] = 1\n",
    "            conditions_1_rows[\"Extract/Buffer\"] = 1\n",
    "            conditions_1_rows[\"DNA-Mix\"] = 1\n",
    "            conditions.append(conditions_1_rows)\n",
    "        else:\n",
    "            conditions_1_rows[\"RNA\"] = 3000\n",
    "            conditions_1_rows[\"Enzyme BZD\"] = 10\n",
    "            conditions_1_rows[\"Cocaine\"] = 100000\n",
    "            conditions_1_rows[\"Extract/Buffer\"] = 80\n",
    "            conditions_1_rows[\"DNA-Mix\"] = 50\n",
    "            conditions.append(conditions_1_rows)\n",
    "\n",
    "        conditions_2_rows = {}\n",
    "        conditions_2_rows[\"CocE DNA\"] = round(float(row[0]), 5)\n",
    "        conditions_2_rows[\"Benzamid\"] = round(float(row[1]), 5)\n",
    "        conditions_2_rows[\"Hippurate\"] = round(float(row[2]), 5)\n",
    "\n",
    "        if normalisation is True:\n",
    "            conditions_2_rows[\"RNA\"] = 0\n",
    "            conditions_2_rows[\"Enzyme BZD\"] = 1\n",
    "            conditions_2_rows[\"Cocaine\"] = 1\n",
    "            conditions_2_rows[\"Extract/Buffer\"] = 1\n",
    "            conditions_2_rows[\"DNA-Mix\"] = 1\n",
    "            conditions.append(conditions_2_rows)\n",
    "        else:\n",
    "            conditions_2_rows[\"RNA\"] = 0\n",
    "            conditions_2_rows[\"Enzyme BZD\"] = 10\n",
    "            conditions_2_rows[\"Cocaine\"] = 100000\n",
    "            conditions_2_rows[\"Extract/Buffer\"] = 80\n",
    "            conditions_2_rows[\"DNA-Mix\"] = 50\n",
    "            conditions.append(conditions_2_rows)\n",
    "\n",
    "        conditions_3_rows = {}\n",
    "        conditions_3_rows[\"CocE DNA\"] = round(float(row[0]), 5)\n",
    "        conditions_3_rows[\"Benzamid\"] = round(float(row[1]), 5)\n",
    "        conditions_3_rows[\"Hippurate\"] = round(float(row[2]), 5)\n",
    "        \n",
    "        if normalisation is True:\n",
    "            conditions_3_rows[\"RNA\"] = 1\n",
    "            conditions_3_rows[\"Enzyme BZD\"] = 0\n",
    "            conditions_3_rows[\"Cocaine\"] = 1\n",
    "            conditions_3_rows[\"Extract/Buffer\"] = 1\n",
    "            conditions_3_rows[\"DNA-Mix\"] = 1\n",
    "            conditions.append(conditions_3_rows)\n",
    "        else:\n",
    "            conditions_3_rows[\"RNA\"] = 3000\n",
    "            conditions_3_rows[\"Enzyme BZD\"] = 0\n",
    "            conditions_3_rows[\"Cocaine\"] = 100000\n",
    "            conditions_3_rows[\"Extract/Buffer\"] = 80\n",
    "            conditions_3_rows[\"DNA-Mix\"] = 50\n",
    "            conditions.append(conditions_3_rows)\n",
    "\n",
    "        conditions_4_rows = {}\n",
    "        conditions_4_rows[\"CocE DNA\"] = round(float(row[0]), 5)\n",
    "        conditions_4_rows[\"Benzamid\"] = round(float(row[1]), 5)\n",
    "        conditions_4_rows[\"Hippurate\"] = round(float(row[2]), 5)\n",
    "\n",
    "        if normalisation is True:\n",
    "            conditions_4_rows[\"RNA\"] = 1\n",
    "            conditions_4_rows[\"Enzyme BZD\"] = 1\n",
    "            conditions_4_rows[\"Cocaine\"] = 0\n",
    "            conditions_4_rows[\"Extract/Buffer\"] = 1\n",
    "            conditions_4_rows[\"DNA-Mix\"] = 1\n",
    "            conditions.append(conditions_4_rows)\n",
    "        else:\n",
    "            conditions_4_rows[\"RNA\"] = 3000\n",
    "            conditions_4_rows[\"Enzyme BZD\"] = 10\n",
    "            conditions_4_rows[\"Cocaine\"] = 0\n",
    "            conditions_4_rows[\"Extract/Buffer\"] = 80\n",
    "            conditions_4_rows[\"DNA-Mix\"] = 50\n",
    "            conditions.append(conditions_4_rows)\n",
    "\n",
    "        conditions_5_rows = {}\n",
    "        conditions_5_rows[\"CocE DNA\"] = round(float(row[0]), 5)\n",
    "        conditions_5_rows[\"Benzamid\"] = round(float(row[1]), 5)\n",
    "        conditions_5_rows[\"Hippurate\"] = round(float(row[2]), 5)\n",
    "\n",
    "        if normalisation is True:\n",
    "            conditions_5_rows[\"RNA\"] = 0\n",
    "            conditions_5_rows[\"Enzyme BZD\"] = 0\n",
    "            conditions_5_rows[\"Cocaine\"] = 1\n",
    "            conditions_5_rows[\"Extract/Buffer\"] = 1\n",
    "            conditions_5_rows[\"DNA-Mix\"] = 1\n",
    "            conditions.append(conditions_5_rows)\n",
    "        else:\n",
    "            conditions_5_rows[\"RNA\"] = 0\n",
    "            conditions_5_rows[\"Enzyme BZD\"] = 0\n",
    "            conditions_5_rows[\"Cocaine\"] = 100000\n",
    "            conditions_5_rows[\"Extract/Buffer\"] = 80\n",
    "            conditions_5_rows[\"DNA-Mix\"] = 50\n",
    "            conditions.append(conditions_5_rows)\n",
    "\n",
    "        conditions_6_rows = {}\n",
    "        conditions_6_rows[\"CocE DNA\"] = round(float(row[0]), 5)\n",
    "        conditions_6_rows[\"Benzamid\"] = round(float(row[1]), 5)\n",
    "        conditions_6_rows[\"Hippurate\"] = round(float(row[2]), 5)\n",
    "\n",
    "        if normalisation is True:\n",
    "            conditions_6_rows[\"RNA\"] = 1\n",
    "            conditions_6_rows[\"Enzyme BZD\"] = 0\n",
    "            conditions_6_rows[\"Cocaine\"] = 0\n",
    "            conditions_6_rows[\"Extract/Buffer\"] = 1\n",
    "            conditions_6_rows[\"DNA-Mix\"] = 1\n",
    "            conditions.append(conditions_6_rows)\n",
    "        else:\n",
    "            conditions_6_rows[\"RNA\"] = 3000\n",
    "            conditions_6_rows[\"Enzyme BZD\"] = 0\n",
    "            conditions_6_rows[\"Cocaine\"] = 0\n",
    "            conditions_6_rows[\"Extract/Buffer\"] = 80\n",
    "            conditions_6_rows[\"DNA-Mix\"] = 50\n",
    "            conditions.append(conditions_6_rows)\n",
    "\n",
    "        conditions_7_rows = {}\n",
    "        conditions_7_rows[\"CocE DNA\"] = round(float(row[0]), 5)\n",
    "        conditions_7_rows[\"Benzamid\"] = round(float(row[1]), 5)\n",
    "        conditions_7_rows[\"Hippurate\"] = round(float(row[2]), 5)\n",
    "\n",
    "        if normalisation is True:\n",
    "            conditions_7_rows[\"RNA\"] = 0\n",
    "            conditions_7_rows[\"Enzyme BZD\"] = 0\n",
    "            conditions_7_rows[\"Cocaine\"] = 0\n",
    "            conditions_7_rows[\"Extract/Buffer\"] = 1\n",
    "            conditions_7_rows[\"DNA-Mix\"] = 1\n",
    "            conditions.append(conditions_7_rows)\n",
    "        else:\n",
    "            conditions_7_rows[\"RNA\"] = 0\n",
    "            conditions_7_rows[\"Enzyme BZD\"] = 0\n",
    "            conditions_7_rows[\"Cocaine\"] = 0\n",
    "            conditions_7_rows[\"Extract/Buffer\"] = 80\n",
    "            conditions_7_rows[\"DNA-Mix\"] = 50\n",
    "            conditions.append(conditions_7_rows)\n",
    "\n",
    "        conditions_8_rows = {}\n",
    "        conditions_8_rows[\"CocE DNA\"] = round(float(row[0]), 5)\n",
    "        conditions_8_rows[\"Benzamid\"] = round(float(row[1]), 5)\n",
    "        conditions_8_rows[\"Hippurate\"] = round(float(row[2]), 5)\n",
    "\n",
    "        if normalisation is True:\n",
    "            conditions_8_rows[\"RNA\"] = 0\n",
    "            conditions_8_rows[\"Enzyme BZD\"] = 1\n",
    "            conditions_8_rows[\"Cocaine\"] = 0\n",
    "            conditions_8_rows[\"Extract/Buffer\"] = 1\n",
    "            conditions_8_rows[\"DNA-Mix\"] = 1\n",
    "            conditions.append(conditions_8_rows)\n",
    "        else:\n",
    "            conditions_8_rows[\"RNA\"] = 0\n",
    "            conditions_8_rows[\"Enzyme BZD\"] = 10\n",
    "            conditions_8_rows[\"Cocaine\"] = 0\n",
    "            conditions_8_rows[\"Extract/Buffer\"] = 80\n",
    "            conditions_8_rows[\"DNA-Mix\"] = 50\n",
    "            conditions.append(conditions_8_rows)\n",
    "\n",
    "    with open(output_file, \"w\") as csv_handle:\n",
    "        csv_writer = csv_DictWriter(\n",
    "            csv_handle,\n",
    "            fieldnames,\n",
    "            restval='',\n",
    "            extrasaction='ignore')\n",
    "        csv_writer.writeheader()\n",
    "        for result in conditions:\n",
    "            csv_writer.writerow(result)\n",
    "\n",
    "# Call function\n",
    "basename = \"conditions_to_test\"\n",
    "save_conditions_to_test(\n",
    "    conditions_to_test,\n",
    "    output_file = \"{}.csv\".format(basename),\n",
    "    normalisation = True)\n",
    "save_conditions_to_test(\n",
    "    conditions_to_test_exploration,\n",
    "    output_file = \"{}_exploration.csv\".format(basename),\n",
    "    normalisation = True)\n",
    "save_conditions_to_test(\n",
    "    conditions_to_test_exploitation,\n",
    "    output_file = \"{}_exploitation.csv\".format(basename),\n",
    "    normalisation = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvements\n",
    "---\n",
    "- Generating the training the dataset should be more generic.\n",
    "- Implement functions from the \"plates generator module\" to generate samples instead of using new functions here.\n",
    "- Just like plates generator and echo instructor, the user should be able to provide his parameter, maximum concentrations and ratios in files instead of writing them directly here.\n",
    "- Saving conditions to test functions can be improved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "bef406ae0ad1ee1b77d8e3df0c6c97cb16cb381fd7d33d12af4c9f531e551d56"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
